{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Collect CT data from LITS Challenge and convert to .png"
      ],
      "metadata": {
        "id": "6J9PdcKteZKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!rsync -ah --progress \"/content/drive/My Drive/LITS Challenge/Training Batch 2\" /content/lits_raw\n",
        "\n",
        "data_dir = \"/content/lits_raw/Training Batch 2\"\n",
        "out_dir = '/content/lits_png'\n",
        "max_slices = 1000\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def normalize_ct(x):\n",
        "    x = np.clip(x, -200, 300)\n",
        "    x = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    return (x * 255).astype(np.uint8)\n",
        "\n",
        "def process_volume(file_number):\n",
        "    seg_path = os.path.join(data_dir, f\"segmentation-{file_number}.nii\")\n",
        "    vol_path = os.path.join(data_dir, f\"volume-{file_number}.nii\")\n",
        "\n",
        "    if not (os.path.exists(seg_path) and os.path.exists(vol_path)):\n",
        "        return f\"Skipped {file_number}\"\n",
        "\n",
        "    seg_nii = nib.load(seg_path)\n",
        "    vol_nii = nib.load(vol_path)\n",
        "\n",
        "    seg_proxy = seg_nii.dataobj\n",
        "    vol_proxy = vol_nii.dataobj\n",
        "\n",
        "    depth = seg_nii.shape[2]\n",
        "    num_slices = min(max_slices, depth)\n",
        "    slice_indices = np.linspace(0, depth - 1, num_slices, dtype=int)\n",
        "\n",
        "    img_dir = f\"{out_dir}/images/{file_number}\"\n",
        "    mask_dir = f\"{out_dir}/masks/{file_number}\"\n",
        "    os.makedirs(img_dir, exist_ok=True)\n",
        "    os.makedirs(mask_dir, exist_ok=True)\n",
        "\n",
        "    for i in slice_indices:\n",
        "        img = normalize_ct(np.array(vol_proxy[:, :, i], dtype=np.float32))\n",
        "        mask = np.array(seg_proxy[:, :, i], dtype=np.uint8)\n",
        "\n",
        "        cv2.imwrite(f\"{img_dir}/{i}.png\", img)\n",
        "        cv2.imwrite(f\"{mask_dir}/{i}.png\", mask)\n",
        "\n",
        "    return f\"Done {file_number}\"\n",
        "\n",
        "# ðŸ”Ž Automatically find all volumes\n",
        "file_numbers = sorted([\n",
        "    int(f.split('-')[1].split('.')[0])\n",
        "    for f in os.listdir(data_dir)\n",
        "    if f.startswith(\"volume-\")\n",
        "])\n",
        "\n",
        "print(\"Found volumes:\", len(file_numbers))\n",
        "\n",
        "# ðŸš€ Parallel processing\n",
        "num_workers = max(1, cpu_count() - 1)\n",
        "print(\"Using workers:\", num_workers)\n",
        "\n",
        "with Pool(num_workers) as p:\n",
        "    results = p.map(process_volume, file_numbers)\n",
        "\n",
        "print(\"\\n\".join(results))\n",
        "print(\"All volumes processed âœ…\")"
      ],
      "metadata": {
        "id": "7iaiOoQZc99O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move Files to /content folder from MyDrive"
      ],
      "metadata": {
        "id": "_hb2skoiePLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rsync -ah --progress \"/content/drive/MyDrive/lits_png.zip\" /content/lits_png.zip"
      ],
      "metadata": {
        "id": "CdMlw4eW-ysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class"
      ],
      "metadata": {
        "id": "OTVyaFy29yxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "67HK26hd9tck"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class LiverCTDataset(Dataset):\n",
        "    def __init__(self, root_dir, output_size=160):\n",
        "        \"\"\"\n",
        "        Dataset for liver CT segmentation.\n",
        "\n",
        "        Loads grayscale CT images and binary segmentation masks from a directory\n",
        "        structure, resizes them to a fixed output size, and returns them as tensors.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        root_dir : str\n",
        "            Path to the root directory containing 'images' and 'masks' folders.\n",
        "        output_size : int, optional\n",
        "            Target spatial dimension for resizing images and masks. Default is 160.\n",
        "        \"\"\"\n",
        "        self.img_dir = os.path.join(root_dir, \"images\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # List of (image_path, mask_path) tuples\n",
        "        self.samples = []\n",
        "\n",
        "        # Walk through case folders to pair images with masks\n",
        "        for case in sorted(os.listdir(self.img_dir)):\n",
        "            case_img_dir = os.path.join(self.img_dir, case)\n",
        "            case_mask_dir = os.path.join(self.mask_dir, case)\n",
        "\n",
        "            # Skip non-directory entries\n",
        "            if not os.path.isdir(case_img_dir):\n",
        "                continue\n",
        "\n",
        "            # Match each image file with its corresponding mask\n",
        "            for file in sorted(os.listdir(case_img_dir)):\n",
        "                img_path = os.path.join(case_img_dir, file)\n",
        "                mask_path = os.path.join(case_mask_dir, file)\n",
        "\n",
        "                # Only include samples where both image and mask exist\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.samples.append((img_path, mask_path))\n",
        "\n",
        "        print(\"Total slices found:\", len(self.samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load and preprocess a single sample.\n",
        "\n",
        "        Reads grayscale image and mask from disk, resizes both to\n",
        "        (output_size, output_size), normalizes the image to [0, 1],\n",
        "        and binarizes the mask.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Index of the sample to retrieve.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        mask : torch.Tensor\n",
        "            Binary segmentation mask of shape (1, output_size, output_size).\n",
        "        img : torch.Tensor\n",
        "            Normalized CT image of shape (1, output_size, output_size).\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If image or mask file cannot be loaded.\n",
        "        \"\"\"\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "\n",
        "        # Load as grayscale numpy arrays\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "        if mask is None:\n",
        "            raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
        "\n",
        "        # Resize image with bilinear interpolation for smooth scaling\n",
        "        img = cv2.resize(img, (self.output_size, self.output_size), interpolation=cv2.INTER_LINEAR)\n",
        "        # Resize mask with nearest-neighbor to preserve binary labels\n",
        "        mask = cv2.resize(mask, (self.output_size, self.output_size), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Convert to tensors and normalize image to [0, 1]\n",
        "        img = torch.from_numpy(img).unsqueeze(0).float() / 255.0\n",
        "        # Binarize mask: any non-zero pixel becomes 1.0\n",
        "        mask = (torch.from_numpy(mask).unsqueeze(0).float() / 255.0 > 0).float()\n",
        "\n",
        "        return mask, img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator and Discriminator Networks"
      ],
      "metadata": {
        "id": "SvXzeD0bdHE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        \"\"\"\n",
        "        Generator model for.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels: int\n",
        "        Number of input channels.\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Define layers for the generator\n",
        "        self.layer1 = self.conv_block(input_channels, 64, 4, 2, 1)\n",
        "        self.layer2 = self.conv_block(64, 128, 4, 2, 1, batch_norm=True)\n",
        "        self.layer3 = self.conv_block(128, 256, 4, 2, 1, batch_norm=True)\n",
        "        self.layer4 = self.conv_block(256, 512, 4, 2, 1, batch_norm=True)\n",
        "        self.layer5 = self.conv_block(512, 256, 3, 1, 1, batch_norm=True)\n",
        "        self.layer6 = self.conv_block(256, 128, 3, 1, 1, batch_norm=True)\n",
        "        self.layer7 = self.conv_block(128, 64, 3, 1, 1, batch_norm=True)\n",
        "        self.layer8 = self.conv_block(64, 1, 3, 1, 1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm=False):\n",
        "        \"\"\"\n",
        "        Defines a convolutional block consisting of convolutional, activation, and optional batch normalization layers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels: int\n",
        "            Number of input channels.\n",
        "        out_channels: int\n",
        "            Number of output channels.\n",
        "        kernel_size: int\n",
        "            Size of the convolutional kernel.\n",
        "        stride: int\n",
        "            Stride value for the convolution operation.\n",
        "        padding: int\n",
        "            Padding value for the convolution operation.\n",
        "        batch_norm: bool, optional\n",
        "            Whether to include batch normalization layer, default is False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: nn.Sequential\n",
        "            Sequential container for the convolutional block layers.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        # Add convolutional layer\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
        "\n",
        "        #  Add ReLU activation function\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Add batch normalization layer if specified\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # Define the forward pass through the generator\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the generator network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            Input tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x8: torch.Tensor\n",
        "            Output tensor after passing through the generator.\n",
        "        \"\"\"\n",
        "\n",
        "        x1 = self.layer1(x)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "        x5 = self.layer5(x4)\n",
        "        x5 = nn.functional.interpolate(x5, scale_factor=2, mode='nearest')\n",
        "        x6 = self.layer6(x5)\n",
        "        x6 = nn.functional.interpolate(x6, scale_factor=2, mode='nearest')\n",
        "        x7 = self.layer7(x6)\n",
        "        x7 = nn.functional.interpolate(x7, scale_factor=2, mode='nearest')\n",
        "        x8 = self.layer8(x7)\n",
        "        x8 = nn.functional.interpolate(x8, scale_factor=2, mode='nearest')\n",
        "\n",
        "        return x8\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        \"\"\"\n",
        "        Discriminator model for.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels: int\n",
        "            Number of input channels.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Define layers for the discriminator\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_channels,\n",
        "                      out_channels=128,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(in_channels=128,\n",
        "                      out_channels=256,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=1,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer_idx=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the discriminator network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            Input tensor.\n",
        "        layer_idx : int or None, optional\n",
        "            Index of the layer up to which features are extracted.\n",
        "            If None, returns the output of the discriminator.\n",
        "            Default is None.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        flattened_features : torch.Tensor\n",
        "            Output tensor after passing through the discriminator.\n",
        "            If `layer_idx` is specified, returns flattened features up to the specified layer.\n",
        "        \"\"\"\n",
        "        if layer_idx is None:\n",
        "            # Normal forward pass through discriminator\n",
        "            x = self.model(x)\n",
        "            return x\n",
        "        else:\n",
        "            # List to extract features up to the specified layer\n",
        "            hierarchical_features = []\n",
        "\n",
        "            for i in range(layer_idx + 1):\n",
        "                x = self.model[i * 3:i * 3 + 3](x)\n",
        "                hierarchical_features.append(x)\n",
        "\n",
        "            # Concatenate the flattened tensors along dim 1 dimension to allow for ease of calculate of the loss\n",
        "            flattened_features = torch.cat([torch.flatten(f, start_dim=1) for f in hierarchical_features], dim=1)\n",
        "\n",
        "            return flattened_features\n",
        "\n",
        "    def forward_all_layers(self, x):\n",
        "        \"\"\"Extract features from all layers in a single forward pass.\"\"\"\n",
        "        features = []\n",
        "        for i in range(3):\n",
        "            x = self.model[i * 3:i * 3 + 3](x)\n",
        "            features.append(x)\n",
        "        return features"
      ],
      "metadata": {
        "id": "9VzshWS99816"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class L1Loss(nn.Module):\n",
        "    def __init__(self, critic_network, num_layers, num_training_images):\n",
        "        \"\"\"\n",
        "        Multi-layer perceptual L1 loss function.\n",
        "\n",
        "        Uses the discriminator's hierarchical features to compute L1 distance\n",
        "        between masked predictions and masked ground truth at multiple network\n",
        "        depths, encouraging perceptually realistic segmentations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        critic_network : nn.Module\n",
        "            Discriminator network with a `forward_all_layers` method that\n",
        "            returns a list of feature maps from each layer.\n",
        "        num_layers : int\n",
        "            Number of layers in the discriminator to extract features from.\n",
        "        num_training_images : int\n",
        "            Batch size (kept for API compatibility).\n",
        "        \"\"\"\n",
        "        super(L1Loss, self).__init__()\n",
        "        self.critic_network = critic_network\n",
        "        self.num_layers = num_layers\n",
        "        self.num_training_images = num_training_images\n",
        "\n",
        "    def forward(self, original_images, predicted_labels, ground_truth_labels):\n",
        "        \"\"\"\n",
        "        Compute the multi-layer perceptual L1 loss.\n",
        "\n",
        "        Masks the original images by both predicted and ground truth labels,\n",
        "        extracts hierarchical features from the discriminator for each,\n",
        "        and computes the average L1 distance across all layers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        original_images : torch.Tensor\n",
        "            Input CT images of shape (B, 1, H, W).\n",
        "        predicted_labels : torch.Tensor\n",
        "            Predicted segmentation masks of shape (B, 1, H, W).\n",
        "        ground_truth_labels : torch.Tensor\n",
        "            Ground truth segmentation masks of shape (B, 1, H, W).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        average_loss : torch.Tensor\n",
        "            Mean L1 loss averaged across all discriminator layers.\n",
        "        \"\"\"\n",
        "        # Extract features from all discriminator layers in a single forward pass\n",
        "        features_predicted = self.critic_network.forward_all_layers(original_images * predicted_labels)\n",
        "        features_ground_truth = self.critic_network.forward_all_layers(original_images * ground_truth_labels)\n",
        "\n",
        "        # Accumulate L1 distance across each layer\n",
        "        total_loss = 0.0\n",
        "        for feat_pred, feat_gt in zip(features_predicted, features_ground_truth):\n",
        "            # Flatten spatial dimensions for element-wise comparison\n",
        "            flat_pred = torch.flatten(feat_pred, start_dim=1)\n",
        "            flat_gt = torch.flatten(feat_gt, start_dim=1)\n",
        "            # Mean Absolute Error between feature representations\n",
        "            total_loss += (flat_pred - flat_gt).abs().mean()\n",
        "\n",
        "        # Average over the number of discriminator layers\n",
        "        average_loss = total_loss / self.num_layers\n",
        "        return average_loss\n",
        "\n",
        "    def calculate_mae_loss(self, features_x, features_x_prime):\n",
        "        \"\"\"\n",
        "        Calculate the L1 norm between two feature maps.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        features_x : torch.Tensor\n",
        "            Feature maps from the first set.\n",
        "        features_x_prime : torch.Tensor\n",
        "            Feature maps from the second set.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        l1_distance : torch.Tensor\n",
        "            L1 norm between the two sets of feature maps.\n",
        "        \"\"\"\n",
        "        l1_distance = torch.norm(torch.abs(features_x - features_x_prime))\n",
        "        return l1_distance"
      ],
      "metadata": {
        "id": "b0bc5lRm_zIP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Train Script"
      ],
      "metadata": {
        "id": "EvB4Qrg-erzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.amp import autocast, GradScaler\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# DATASET + TRAIN/VAL SPLIT\n",
        "# -------------------------\n",
        "data_root = \"/content/lits_png\"\n",
        "save_dir = \"/content/drive/MyDrive/checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "full_dataset = LiverCTDataset(data_root)\n",
        "\n",
        "val_ratio = 0.15\n",
        "val_size = int(len(full_dataset) * val_ratio)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# MODELS\n",
        "# -------------------------\n",
        "image_channels = 1\n",
        "num_discriminator_layers = 3\n",
        "\n",
        "generator = Generator(input_channels=image_channels).to(device)\n",
        "discriminator = Discriminator(input_channels=image_channels).to(device)\n",
        "\n",
        "generator = torch.compile(generator)\n",
        "discriminator = torch.compile(discriminator)\n",
        "\n",
        "# -------------------------\n",
        "# LOSS\n",
        "# -------------------------\n",
        "criterion = L1Loss(\n",
        "    critic_network=discriminator,\n",
        "    num_layers=num_discriminator_layers,\n",
        "    num_training_images=batch_size\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# OPTIMIZERS\n",
        "# -------------------------\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# -------------------------\n",
        "# MIXED PRECISION\n",
        "# -------------------------\n",
        "scaler = GradScaler()\n",
        "\n",
        "# -------------------------\n",
        "# VALIDATION\n",
        "# -------------------------\n",
        "def validate(generator, val_loader, criterion, device):\n",
        "    generator.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for real_masks, real_images in val_loader:\n",
        "            real_masks = real_masks.to(device, non_blocking=True)\n",
        "            real_images = real_images.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                generated_masks = generator(real_images)\n",
        "                loss = criterion(real_images, generated_masks, real_masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    generator.train()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "# -------------------------\n",
        "# TRAINING LOOP\n",
        "# -------------------------\n",
        "num_epochs = 7\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "\n",
        "    for batch_idx, (real_masks, real_images) in enumerate(train_loader):\n",
        "\n",
        "        real_masks = real_masks.to(device, non_blocking=True)\n",
        "        real_images = real_images.to(device, non_blocking=True)\n",
        "\n",
        "        # -----------------\n",
        "        # Train Discriminator\n",
        "        # -----------------\n",
        "        discriminator_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            fake_masks = generator(real_images)\n",
        "\n",
        "            feat_real = discriminator.forward_all_layers(real_images * real_masks)\n",
        "            feat_fake = discriminator.forward_all_layers(real_images * fake_masks.detach())\n",
        "\n",
        "            d_loss = 0.0\n",
        "            for fr, ff in zip(feat_real, feat_fake):\n",
        "                d_loss += (torch.flatten(fr, 1) - torch.flatten(ff, 1)).abs().mean()\n",
        "            d_loss = d_loss / num_discriminator_layers\n",
        "\n",
        "        scaler.scale(d_loss).backward()\n",
        "        scaler.step(discriminator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # -----------------\n",
        "        # Train Generator\n",
        "        # -----------------\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        generator_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            generated_masks = generator(real_images)\n",
        "\n",
        "            # Adversarial loss from discriminator\n",
        "            g_adv_loss = criterion(real_images, generated_masks, real_masks)\n",
        "\n",
        "            # Direct pixel loss (always provides gradient)\n",
        "            g_pixel_loss = (generated_masks - real_masks).abs().mean()\n",
        "\n",
        "            # Combined: pixel loss dominates early, adversarial refines later\n",
        "            g_loss = g_pixel_loss + 0.01 * g_adv_loss\n",
        "\n",
        "        scaler.scale(g_loss).backward()\n",
        "        scaler.step(generator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "                f\"Batch [{batch_idx}/{len(train_loader)}] \"\n",
        "                f\"D Loss: {d_loss.item():.4f} \"\n",
        "                f\"G Loss: {g_loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    # -------------------------\n",
        "    # VALIDATION\n",
        "    # -------------------------\n",
        "    avg_train_loss = epoch_g_loss / len(train_loader)\n",
        "    avg_val_loss = validate(generator, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] Train G Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # SAVE BEST MODEL\n",
        "    # -------------------------\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'generator_optimizer': generator_optimizer.state_dict(),\n",
        "            'discriminator_optimizer': discriminator_optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "        print(f\"âœ… Best model saved at epoch {epoch} with Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "     # -------------------------\n",
        "    # SAVE VALIDATION PREDICTIONS (every epoch)\n",
        "    # -------------------------\n",
        "    vis_dir = os.path.join(save_dir, \"val_predictions\", f\"epoch_{epoch:04d}\")\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample_idx = 0\n",
        "        for val_masks, val_images in val_loader:\n",
        "            val_images = val_images.to(device, non_blocking=True)\n",
        "            with autocast(device_type='cuda'):\n",
        "                val_preds = generator(val_images)\n",
        "\n",
        "            for i in range(val_images.size(0)):\n",
        "                x = val_images[i].cpu().squeeze().numpy()\n",
        "                y = val_masks[i].cpu().squeeze().numpy()\n",
        "                z = val_preds[i].cpu().squeeze().numpy()\n",
        "\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "                axes[0].set_title(\"CT\")\n",
        "                axes[0].imshow(x, cmap=\"gray\")\n",
        "                axes[0].axis(\"off\")\n",
        "                axes[1].set_title(\"GT Mask\")\n",
        "                axes[1].imshow(y, cmap=\"gray\")\n",
        "                axes[1].axis(\"off\")\n",
        "                axes[2].set_title(\"Pred Mask\")\n",
        "                axes[2].imshow(z, cmap=\"gray\")\n",
        "                axes[2].axis(\"off\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(vis_dir, f\"sample_{sample_idx:04d}.png\"), dpi=100, bbox_inches='tight')\n",
        "                plt.close('all')\n",
        "\n",
        "                sample_idx += 1\n",
        "                if sample_idx >= 100:  # only save first 100\n",
        "                    break\n",
        "            if sample_idx >= 100:  # only save first 100\n",
        "                break\n",
        "\n",
        "    print(f\"ðŸ“ Saved {sample_idx} validation predictions to {vis_dir}\")\n",
        "\n",
        "    # Plot loss curves\n",
        "    if len(train_losses) > 1:\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        ax.plot(train_losses, label='Train Loss')\n",
        "        ax.plot(val_losses, label='Val Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('G Loss')\n",
        "        ax.legend()\n",
        "        ax.set_title('Training vs Validation Loss')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, \"loss_curve.png\"), dpi=100)\n",
        "        plt.show()\n",
        "        plt.close('all')"
      ],
      "metadata": {
        "id": "VuaHWQB3_GrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Resume Training from Checkpoint"
      ],
      "metadata": {
        "id": "JmlFyyTxew9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.amp import autocast, GradScaler\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# DATASET + TRAIN/VAL SPLIT\n",
        "# -------------------------\n",
        "data_root = \"/content/lits_png\"\n",
        "save_dir = \"/content/drive/MyDrive/checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "full_dataset = LiverCTDataset(data_root)\n",
        "\n",
        "val_ratio = 0.15\n",
        "val_size = int(len(full_dataset) * val_ratio)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# MODELS\n",
        "# -------------------------\n",
        "image_channels = 1\n",
        "num_discriminator_layers = 3\n",
        "\n",
        "generator = Generator(input_channels=image_channels).to(device)\n",
        "discriminator = Discriminator(input_channels=image_channels).to(device)\n",
        "\n",
        "generator = torch.compile(generator)\n",
        "discriminator = torch.compile(discriminator)\n",
        "\n",
        "# -------------------------\n",
        "# LOSS\n",
        "# -------------------------\n",
        "criterion = L1Loss(\n",
        "    critic_network=discriminator,\n",
        "    num_layers=num_discriminator_layers,\n",
        "    num_training_images=batch_size\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# OPTIMIZERS\n",
        "# -------------------------\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# -------------------------\n",
        "# MIXED PRECISION\n",
        "# -------------------------\n",
        "scaler = GradScaler()\n",
        "\n",
        "# -------------------------\n",
        "# LOAD CHECKPOINT\n",
        "# -------------------------\n",
        "checkpoint = torch.load(os.path.join(save_dir, \"best_model.pth\"), map_location=device)\n",
        "\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "generator_optimizer.load_state_dict(checkpoint['generator_optimizer'])\n",
        "discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer'])\n",
        "\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "best_val_loss = checkpoint['val_loss']\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"âœ… Loaded checkpoint from epoch {checkpoint['epoch']} with Val Loss: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# VALIDATION\n",
        "# -------------------------\n",
        "def validate(generator, val_loader, criterion, device):\n",
        "    generator.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for real_masks, real_images in val_loader:\n",
        "            real_masks = real_masks.to(device, non_blocking=True)\n",
        "            real_images = real_images.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                generated_masks = generator(real_images)\n",
        "                loss = criterion(real_images, generated_masks, real_masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    generator.train()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "# -------------------------\n",
        "# RESUME TRAINING\n",
        "# -------------------------\n",
        "num_epochs = 7\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "\n",
        "    for batch_idx, (real_masks, real_images) in enumerate(train_loader):\n",
        "\n",
        "        real_masks = real_masks.to(device, non_blocking=True)\n",
        "        real_images = real_images.to(device, non_blocking=True)\n",
        "\n",
        "        # -----------------\n",
        "        # Train Discriminator\n",
        "        # -----------------\n",
        "        discriminator_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            fake_masks = generator(real_images)\n",
        "\n",
        "            feat_real = discriminator.forward_all_layers(real_images * real_masks)\n",
        "            feat_fake = discriminator.forward_all_layers(real_images * fake_masks.detach())\n",
        "\n",
        "            d_loss = 0.0\n",
        "            for fr, ff in zip(feat_real, feat_fake):\n",
        "                d_loss += (torch.flatten(fr, 1) - torch.flatten(ff, 1)).abs().mean()\n",
        "            d_loss = d_loss / num_discriminator_layers\n",
        "\n",
        "        scaler.scale(d_loss).backward()\n",
        "        scaler.step(discriminator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # -----------------\n",
        "        # Train Generator\n",
        "        # -----------------\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        generator_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(device_type='cuda'):\n",
        "            generated_masks = generator(real_images)\n",
        "            g_adv_loss = criterion(real_images, generated_masks, real_masks)\n",
        "            g_pixel_loss = (generated_masks - real_masks).abs().mean()\n",
        "            g_loss = g_pixel_loss + 0.01 * g_adv_loss\n",
        "\n",
        "        scaler.scale(g_loss).backward()\n",
        "        scaler.step(generator_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "                f\"Batch [{batch_idx}/{len(train_loader)}] \"\n",
        "                f\"D Loss: {d_loss.item():.4f} \"\n",
        "                f\"G Loss: {g_loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    # -------------------------\n",
        "    # VALIDATION\n",
        "    # -------------------------\n",
        "    avg_train_loss = epoch_g_loss / len(train_loader)\n",
        "    avg_val_loss = validate(generator, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] Train G Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # SAVE BEST MODEL\n",
        "    # -------------------------\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'generator_optimizer': generator_optimizer.state_dict(),\n",
        "            'discriminator_optimizer': discriminator_optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "        print(f\"âœ… Best model saved at epoch {epoch} with Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # SAVE VALIDATION PREDICTIONS\n",
        "    # -------------------------\n",
        "    vis_dir = os.path.join(save_dir, \"val_predictions\", f\"epoch_{epoch:04d}\")\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample_idx = 0\n",
        "        for val_masks, val_images in val_loader:\n",
        "            val_images = val_images.to(device, non_blocking=True)\n",
        "            with autocast(device_type='cuda'):\n",
        "                val_preds = generator(val_images)\n",
        "\n",
        "            for i in range(val_images.size(0)):\n",
        "                x = val_images[i].cpu().squeeze().numpy()\n",
        "                y = val_masks[i].cpu().squeeze().numpy()\n",
        "                z = val_preds[i].cpu().squeeze().numpy()\n",
        "\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "                axes[0].set_title(\"CT\")\n",
        "                axes[0].imshow(x, cmap=\"gray\")\n",
        "                axes[0].axis(\"off\")\n",
        "                axes[1].set_title(\"GT Mask\")\n",
        "                axes[1].imshow(y, cmap=\"gray\")\n",
        "                axes[1].axis(\"off\")\n",
        "                axes[2].set_title(\"Pred Mask\")\n",
        "                axes[2].imshow(z, cmap=\"gray\")\n",
        "                axes[2].axis(\"off\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(vis_dir, f\"sample_{sample_idx:04d}.png\"), dpi=100, bbox_inches='tight')\n",
        "                plt.close('all')\n",
        "\n",
        "                sample_idx += 1\n",
        "                if sample_idx >= 100:\n",
        "                    break\n",
        "            if sample_idx >= 100:\n",
        "                break\n",
        "\n",
        "    print(f\"ðŸ“ Saved {sample_idx} validation predictions to {vis_dir}\")\n",
        "\n",
        "    if len(train_losses) > 1:\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        ax.plot(train_losses, label='Train Loss')\n",
        "        ax.plot(val_losses, label='Val Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('G Loss')\n",
        "        ax.legend()\n",
        "        ax.set_title('Training vs Validation Loss')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, \"loss_curve.png\"), dpi=100)\n",
        "        plt.show()\n",
        "        plt.close('all')"
      ],
      "metadata": {
        "id": "_VA5MezMLgSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}